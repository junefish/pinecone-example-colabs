{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/junefish/pinecone-example-colabs/blob/main/gpt_4_langchain_docs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_HDKlQO5svqI"
      },
      "outputs": [],
      "source": [
        "!pip install -qU \\\n",
        "  openai==0.27.7 \\\n",
        "  \"pinecone-client[grpc]\"==2.2.1 \\\n",
        "  pinecone-datasets=='0.5.0rc11' \\\n",
        "  fastapi \\\n",
        "  kaleido \\\n",
        "  python-multipart \\\n",
        "  uvicorn \\\n",
        "  cohere \\\n",
        "  tiktoken \\\n",
        "  typing-extensions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZdWl7kR9KWJ"
      },
      "source": [
        "---\n",
        "\n",
        "üö® _Note: the above `pip install` is formatted for Jupyter notebooks. If running elsewhere you may need to drop the `!`._\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgUEJ6vDum8q"
      },
      "source": [
        "In this example, we will use a pre-embedding dataset of the LangChain docs from [python.langchain.readthedocs.com/](https://python.langchain.com/en/latest/). If you'd like to see how we perform the data preparation refer to [this notebook]().\n",
        "\n",
        "Let's go ahead and download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "phk1tQ0U92DM",
        "outputId": "78e56314-ecc1-44be-8a93-8643229ffc22"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                       id  \\\n",
              "0.0  417ede5d-39be-498f-b518-f47ed4e53b90   \n",
              "1.0  110f550d-110b-4378-b95e-141397fa21bc   \n",
              "2.0  d5f00f02-3295-4567-b297-5e3262dc2728   \n",
              "3.0  0b6fe3c6-1f0e-4608-a950-43231e46b08a   \n",
              "4.0  39d5f15f-b973-42c0-8c9b-a2df49b627dc   \n",
              "\n",
              "                                                values sparse_values  \\\n",
              "0.0  [0.005949743557721376, 0.01983247883617878, -0...          None   \n",
              "1.0  [0.009401749819517136, 0.02443608082830906, 0....          None   \n",
              "2.0  [-0.005517194513231516, 0.0208403542637825, 0....          None   \n",
              "3.0  [-0.006499645300209522, 0.0011573900701478124,...          None   \n",
              "4.0  [-0.005658374633640051, 0.00817849114537239, 0...          None   \n",
              "\n",
              "                                              metadata  \n",
              "0.0  {'chunk': 0, 'text': '.rst\n",
              ".pdf\n",
              "Welcome to Lan...  \n",
              "1.0  {'chunk': 1, 'text': 'Use Cases#\n",
              "Best practice...  \n",
              "2.0  {'chunk': 2, 'text': 'Gallery: A collection of...  \n",
              "3.0  {'chunk': 0, 'text': 'Search\n",
              "Error\n",
              "Please acti...  \n",
              "4.0  {'chunk': 0, 'text': '.md\n",
              ".pdf\n",
              "Dependents\n",
              "Depe...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-dd850886-cce1-4df0-901a-0ef4832b08eb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>values</th>\n",
              "      <th>sparse_values</th>\n",
              "      <th>metadata</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.0</th>\n",
              "      <td>417ede5d-39be-498f-b518-f47ed4e53b90</td>\n",
              "      <td>[0.005949743557721376, 0.01983247883617878, -0...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'text': '.rst\n",
              ".pdf\n",
              "Welcome to Lan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1.0</th>\n",
              "      <td>110f550d-110b-4378-b95e-141397fa21bc</td>\n",
              "      <td>[0.009401749819517136, 0.02443608082830906, 0....</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 1, 'text': 'Use Cases#\n",
              "Best practice...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2.0</th>\n",
              "      <td>d5f00f02-3295-4567-b297-5e3262dc2728</td>\n",
              "      <td>[-0.005517194513231516, 0.0208403542637825, 0....</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 2, 'text': 'Gallery: A collection of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3.0</th>\n",
              "      <td>0b6fe3c6-1f0e-4608-a950-43231e46b08a</td>\n",
              "      <td>[-0.006499645300209522, 0.0011573900701478124,...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'text': 'Search\n",
              "Error\n",
              "Please acti...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4.0</th>\n",
              "      <td>39d5f15f-b973-42c0-8c9b-a2df49b627dc</td>\n",
              "      <td>[-0.005658374633640051, 0.00817849114537239, 0...</td>\n",
              "      <td>None</td>\n",
              "      <td>{'chunk': 0, 'text': '.md\n",
              ".pdf\n",
              "Dependents\n",
              "Depe...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dd850886-cce1-4df0-901a-0ef4832b08eb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-dd850886-cce1-4df0-901a-0ef4832b08eb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-dd850886-cce1-4df0-901a-0ef4832b08eb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-230db9f5-ae6a-4427-82b9-97b11d6d97b3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-230db9f5-ae6a-4427-82b9-97b11d6d97b3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-230db9f5-ae6a-4427-82b9-97b11d6d97b3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "from pinecone_datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset('langchain-python-docs-text-embedding-ada-002')\n",
        "# we drop sparse_values as they are not needed for this example\n",
        "dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
        "dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
        "dataset.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JegURaAg2PuN"
      },
      "source": [
        "Our chunks are ready so now we move onto embedding and indexing everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WPi4MZvMNvUH"
      },
      "source": [
        "## Initializing the Index"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5RRQArrN2lN"
      },
      "source": [
        "Now we need a place to store these embeddings and enable a efficient vector search through them all. To do that we use Pinecone, we can get a [free API key](https://app.pinecone.io/) and enter it below where we will initialize our connection to Pinecone and create a new index."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-dotenv\n",
        "\n",
        "import os\n",
        "import dotenv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "dotenv.load_dotenv('/content/drive/MyDrive/Colab Notebooks/.env')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xmdXgjnw3sk",
        "outputId": "cdb56864-244e-4a0e-cf3e-28e515e25e66"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.0-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.0\n",
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZO5_EdUAum8v"
      },
      "outputs": [],
      "source": [
        "import pinecone\n",
        "\n",
        "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
        "api_key = os.getenv(\"PINECONE_API_KEY\") or \"PINECONE_API_KEY\"\n",
        "# find your environment next to the api key in pinecone console\n",
        "env = os.getenv(\"PINECONE_ENVIRONMENT\") or \"PINECONE_ENVIRONMENT\"\n",
        "\n",
        "pinecone.init(api_key=api_key, environment=env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2GQAnohhum8v",
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "index_name = 'gpt-4-langchain-docs-fast'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EO8sbJFZNyIZ",
        "outputId": "7a9aa5f6-8016-41b3-b20f-b1945f7ae4ce"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 1536,\n",
              " 'index_fullness': 0.0,\n",
              " 'namespaces': {'': {'vector_count': 3476}},\n",
              " 'total_vector_count': 3476}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "# check if index already exists (it shouldn't if this is first time)\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    # if does not exist, create index\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=1536,  # dimensionality of text-embedding-ada-002\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to be initialized\n",
        "    time.sleep(1)\n",
        "\n",
        "# connect to index\n",
        "index = pinecone.GRPCIndex(index_name)\n",
        "# view index stats\n",
        "index.describe_index_stats()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezSTzN2rPa2o"
      },
      "source": [
        "We can see the index is currently empty with a `total_vector_count` of `0`. We can begin populating it with OpenAI `text-embedding-ada-002` built embeddings like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "iZbFbulAPeop"
      },
      "outputs": [],
      "source": [
        "for batch in dataset.iter_documents(batch_size=100):\n",
        "    index.upsert(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YttJOrEtQIF9"
      },
      "source": [
        "Now we've added all of our langchain docs to the index. With that we can move on to retrieval and then answer generation using GPT-4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FumVmMRlQQ7w"
      },
      "source": [
        "## Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nLRODeL-QTJ9"
      },
      "source": [
        "To search through our documents we first need to create a query vector `xq`. Using `xq` we will retrieve the most relevant chunks from the LangChain docs. To create that query vector we must initialize a `text-embedding-ada-002` embedding model with OpenAI. For this, you need an [OpenAI API key](https://platform.openai.com/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "dhDTQnqp-yyx"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "# get api key from platform.openai.com\n",
        "openai.api_key = os.getenv('OPENAI_API_KEY') or 'OPENAI_API_KEY'\n",
        "\n",
        "embed_model = \"text-embedding-ada-002\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FMUPdX9cQQYC"
      },
      "outputs": [],
      "source": [
        "query = \"how do I use the LLMChain in LangChain?\"\n",
        "\n",
        "res = openai.Embedding.create(\n",
        "    input=[query],\n",
        "    engine=embed_model\n",
        ")\n",
        "\n",
        "# retrieve from Pinecone\n",
        "xq = res['data'][0]['embedding']\n",
        "\n",
        "# get relevant contexts (including the questions)\n",
        "res = index.query(xq, top_k=5, include_metadata=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zl9SrFPkQjg-",
        "outputId": "67fbb4bf-e4ef-4b32-b8f5-f80863333271"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'matches': [{'id': '2f66a6a5-c829-4118-acb8-f08667f3f95d',\n",
              "              'metadata': {'chunk': 2.0,\n",
              "                           'text': 'for full documentation on:\\\\n\\\\nGetting '\n",
              "                                   'started (installation, setting up the '\n",
              "                                   'environment, simple examples)\\\\n\\\\nHow-To '\n",
              "                                   'examples (demos, integrations, helper '\n",
              "                                   'functions)\\\\n\\\\nReference (full API '\n",
              "                                   'docs)\\\\n\\\\nResources (high-level '\n",
              "                                   'explanation of core '\n",
              "                                   'concepts)\\\\n\\\\n√∞\\\\x9f\\\\x9a\\\\x80 What can '\n",
              "                                   'this help with?\\\\n\\\\nThere are six main '\n",
              "                                   'areas that LangChain is designed to help '\n",
              "                                   'with.\\\\nThese are, in increasing order of '\n",
              "                                   'complexity:\\\\n\\\\n√∞\\\\x9f‚Äú\\\\x83 LLMs and '\n",
              "                                   'Prompts:\\\\n\\\\nThis includes prompt '\n",
              "                                   'management, prompt optimization, a generic '\n",
              "                                   'interface for all LLMs, and common '\n",
              "                                   'utilities for working with '\n",
              "                                   'LLMs.\\\\n\\\\n√∞\\\\x9f‚Äù\\\\x97 '\n",
              "                                   'Chains:\\\\n\\\\nChains go beyond a single LLM '\n",
              "                                   'call and involve sequences of calls '\n",
              "                                   '(whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\n\\\\n√∞\\\\x9f‚Äú\\\\x9a '\n",
              "                                   'Data Augmented Generation:\\\\n\\\\nData '\n",
              "                                   'Augmented Generation involves specific '\n",
              "                                   'types of chains that first interact with '\n",
              "                                   'an external data source to fetch data for '\n",
              "                                   'use in the generation step. Examples '\n",
              "                                   'include summarization of long pieces of '\n",
              "                                   'text and question/answering over specific '\n",
              "                                   'data sources.\\\\n\\\\n√∞\\\\x9f¬§\\\\x96 '\n",
              "                                   'Agents:\\\\n\\\\nAgents involve an LLM making '\n",
              "                                   'decisions about which Actions to take, '\n",
              "                                   'taking that Action, seeing an Observation, '\n",
              "                                   'and repeating that until done. LangChain',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/markdown.html'},\n",
              "              'score': 0.86979645,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'e26407fd-df5d-4e88-a59e-2ffd2b2b87d6',\n",
              "              'metadata': {'chunk': 17.0,\n",
              "                           'text': 'an Observation, and repeating that until '\n",
              "                                   'done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse '\n",
              "                                   'Cases#\\\\nThe above modules can be used in '\n",
              "                                   'a variety of ways. LangChain also provides '\n",
              "                                   'guidance and assistance in this. Below are '\n",
              "                                   'some of the common use cases LangChain '\n",
              "                                   'supports.\\\\n\\\\nPersonal Assistants: The '\n",
              "                                   'main LangChain use case. Personal '\n",
              "                                   'assistants need to take actions, remember '\n",
              "                                   'interactions, and have knowledge about '\n",
              "                                   'your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to use LLMs to '\n",
              "                                   'query data that is stored in a tabular '\n",
              "                                   'format (csvs, SQL, dataframes, etc) you '\n",
              "                                   'should read this page.\\\\nInteracting with '\n",
              "                                   'APIs: Enabling LLMs to interact with APIs '\n",
              "                                   'is extremely powerful in order to give '\n",
              "                                   'them more up-to-date information and allow '\n",
              "                                   'them to take actions.\\\\nExtraction: '\n",
              "                                   'Extract structured information from '\n",
              "                                   'text.\\\\nSummarization: Summarizing longer '\n",
              "                                   'documents into shorter, more condensed '\n",
              "                                   'chunks of information. A type of Data '\n",
              "                                   'Augmented Generation.\\\\nEvaluation: '\n",
              "                                   'Generative models are notoriously',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.8587569,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'd578f3d3-8107-4fb1-87cd-0c81923761f7',\n",
              "              'metadata': {'chunk': 7.0,\n",
              "                           'text': 'working with raw text, they work with '\n",
              "                                   'messages. LangChain provides a standard '\n",
              "                                   'interface for working with them and doing '\n",
              "                                   'all the same things as '\n",
              "                                   'above.\\\\n\\\\n\\\\n\\\\n\\\\n\\\\nUse Cases#\\\\nThe '\n",
              "                                   'above modules can be used in a variety of '\n",
              "                                   'ways. LangChain also provides guidance and '\n",
              "                                   'assistance in this. Below are some of the '\n",
              "                                   'common use cases LangChain '\n",
              "                                   'supports.\\\\n\\\\nAgents: Agents are systems '\n",
              "                                   'that use a language model to interact with '\n",
              "                                   'other tools. These can be used to do more '\n",
              "                                   'grounded question/answering, interact with '\n",
              "                                   'APIs, or even take actions.\\\\nChatbots: '\n",
              "                                   'Since language models are good at '\n",
              "                                   'producing text, that makes them ideal for '\n",
              "                                   'creating chatbots.\\\\nData Augmented '\n",
              "                                   'Generation: Data Augmented Generation '\n",
              "                                   'involves specific types of chains that '\n",
              "                                   'first interact with an external datasource '\n",
              "                                   'to fetch data to use in the generation '\n",
              "                                   'step. Examples of this include '\n",
              "                                   'summarization of long pieces of text and '\n",
              "                                   'question/answering over specific data '\n",
              "                                   'sources.\\\\nQuestion Answering: Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an answer. A type '\n",
              "                                   'of Data Augmented '\n",
              "                                   'Generation.\\\\nSummarization: Summarizing '\n",
              "                                   'longer documents into shorter, more '\n",
              "                                   'condensed chunks of information. A type of '\n",
              "                                   'Data Augmented Generation.\\\\nQuerying '\n",
              "                                   'Tabular Data: If you want to understand '\n",
              "                                   'how to use LLMs to query data that is '\n",
              "                                   'stored in a tabular format (csvs,',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/sitemap.html'},\n",
              "              'score': 0.85653305,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': '2a19b051-2bf3-4675-ac84-8cfb859c9fc7',\n",
              "              'metadata': {'chunk': 1.0,\n",
              "                           'text': 'Initiate the LLMChain\\n'\n",
              "                                   'Run the LLMChain\\n'\n",
              "                                   'By Harrison Chase\\n'\n",
              "                                   '    \\n'\n",
              "                                   '      ¬© Copyright 2023, Harrison Chase.\\n'\n",
              "                                   '      \\n'\n",
              "                                   '  Last updated on May 30, 2023.',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/models/llms/integrations/pipelineai_example.html'},\n",
              "              'score': 0.85121495,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []},\n",
              "             {'id': 'badbba15-bc60-4177-89c0-f7f743a7c655',\n",
              "              'metadata': {'chunk': 2.0,\n",
              "                           'text': 'use memory.\\\\nIndexes: Language models are '\n",
              "                                   'often more powerful when combined with '\n",
              "                                   'your own text data - this module covers '\n",
              "                                   'best practices for doing exactly '\n",
              "                                   'that.\\\\nChains: Chains go beyond just a '\n",
              "                                   'single LLM call, and are sequences of '\n",
              "                                   'calls (whether to an LLM or a different '\n",
              "                                   'utility). LangChain provides a standard '\n",
              "                                   'interface for chains, lots of integrations '\n",
              "                                   'with other tools, and end-to-end chains '\n",
              "                                   'for common applications.\\\\nAgents: Agents '\n",
              "                                   'involve an LLM making decisions about '\n",
              "                                   'which Actions to take, taking that Action, '\n",
              "                                   'seeing an Observation, and repeating that '\n",
              "                                   'until done. LangChain provides a standard '\n",
              "                                   'interface for agents, a selection of '\n",
              "                                   'agents to choose from, and examples of end '\n",
              "                                   'to end agents.\\\\nUse Cases\\\\nThe above '\n",
              "                                   'modules can be used in a variety of ways. '\n",
              "                                   'LangChain also provides guidance and '\n",
              "                                   'assistance in this. Below are some of the '\n",
              "                                   'common use cases LangChain '\n",
              "                                   'supports.\\\\nPersonal Assistants: The main '\n",
              "                                   'LangChain use case. Personal assistants '\n",
              "                                   'need to take actions, remember '\n",
              "                                   'interactions, and have knowledge about '\n",
              "                                   'your data.\\\\nQuestion Answering: The '\n",
              "                                   'second big LangChain use case. Answering '\n",
              "                                   'questions over specific documents, only '\n",
              "                                   'utilizing the information in those '\n",
              "                                   'documents to construct an '\n",
              "                                   'answer.\\\\nChatbots: Since language models '\n",
              "                                   'are good at producing text, that makes '\n",
              "                                   'them ideal for creating '\n",
              "                                   'chatbots.\\\\nQuerying Tabular Data: If you '\n",
              "                                   'want to understand how to',\n",
              "                           'url': 'https://python.langchain.com/en/latest/modules/indexes/document_loaders/examples/diffbot.html'},\n",
              "              'score': 0.8505416,\n",
              "              'sparse_values': {'indices': [], 'values': []},\n",
              "              'values': []}],\n",
              " 'namespace': ''}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "res"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MoBSiDLIUADZ"
      },
      "source": [
        "With retrieval complete, we move on to feeding these into GPT-4 to produce answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qfzS4-6-UXgX"
      },
      "source": [
        "## Retrieval Augmented Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPC1jQaKUcy0"
      },
      "source": [
        "GPT-4 is currently accessed via the `ChatCompletions` endpoint of OpenAI. To add the information we retrieved into the model, we need to pass it into our user prompts *alongside* our original query. We can do that like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "unZstoHNUHeG"
      },
      "outputs": [],
      "source": [
        "# get list of retrieved text\n",
        "contexts = [item['metadata']['text'] for item in res['matches']]\n",
        "\n",
        "augmented_query = \"\\n\\n---\\n\\n\".join(contexts)+\"\\n\\n-----\\n\\n\"+query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LRcEHm0Z9fXE",
        "outputId": "0ea9121b-92bc-441c-95a1-819a32b7b1f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for full documentation on:\\n\\nGetting started (installation, setting up the environment, simple examples)\\n\\nHow-To examples (demos, integrations, helper functions)\\n\\nReference (full API docs)\\n\\nResources (high-level explanation of core concepts)\\n\\n√∞\\x9f\\x9a\\x80 What can this help with?\\n\\nThere are six main areas that LangChain is designed to help with.\\nThese are, in increasing order of complexity:\\n\\n√∞\\x9f‚Äú\\x83 LLMs and Prompts:\\n\\nThis includes prompt management, prompt optimization, a generic interface for all LLMs, and common utilities for working with LLMs.\\n\\n√∞\\x9f‚Äù\\x97 Chains:\\n\\nChains go beyond a single LLM call and involve sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\n\\n√∞\\x9f‚Äú\\x9a Data Augmented Generation:\\n\\nData Augmented Generation involves specific types of chains that first interact with an external data source to fetch data for use in the generation step. Examples include summarization of long pieces of text and question/answering over specific data sources.\\n\\n√∞\\x9f¬§\\x96 Agents:\\n\\nAgents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain\n",
            "\n",
            "---\n",
            "\n",
            "an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs, SQL, dataframes, etc) you should read this page.\\nInteracting with APIs: Enabling LLMs to interact with APIs is extremely powerful in order to give them more up-to-date information and allow them to take actions.\\nExtraction: Extract structured information from text.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nEvaluation: Generative models are notoriously\n",
            "\n",
            "---\n",
            "\n",
            "working with raw text, they work with messages. LangChain provides a standard interface for working with them and doing all the same things as above.\\n\\n\\n\\n\\n\\nUse Cases#\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\n\\nAgents: Agents are systems that use a language model to interact with other tools. These can be used to do more grounded question/answering, interact with APIs, or even take actions.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nData Augmented Generation: Data Augmented Generation involves specific types of chains that first interact with an external datasource to fetch data to use in the generation step. Examples of this include summarization of long pieces of text and question/answering over specific data sources.\\nQuestion Answering: Answering questions over specific documents, only utilizing the information in those documents to construct an answer. A type of Data Augmented Generation.\\nSummarization: Summarizing longer documents into shorter, more condensed chunks of information. A type of Data Augmented Generation.\\nQuerying Tabular Data: If you want to understand how to use LLMs to query data that is stored in a tabular format (csvs,\n",
            "\n",
            "---\n",
            "\n",
            "Initiate the LLMChain\n",
            "Run the LLMChain\n",
            "By Harrison Chase\n",
            "    \n",
            "      ¬© Copyright 2023, Harrison Chase.\n",
            "      \n",
            "  Last updated on May 30, 2023.\n",
            "\n",
            "---\n",
            "\n",
            "use memory.\\nIndexes: Language models are often more powerful when combined with your own text data - this module covers best practices for doing exactly that.\\nChains: Chains go beyond just a single LLM call, and are sequences of calls (whether to an LLM or a different utility). LangChain provides a standard interface for chains, lots of integrations with other tools, and end-to-end chains for common applications.\\nAgents: Agents involve an LLM making decisions about which Actions to take, taking that Action, seeing an Observation, and repeating that until done. LangChain provides a standard interface for agents, a selection of agents to choose from, and examples of end to end agents.\\nUse Cases\\nThe above modules can be used in a variety of ways. LangChain also provides guidance and assistance in this. Below are some of the common use cases LangChain supports.\\nPersonal Assistants: The main LangChain use case. Personal assistants need to take actions, remember interactions, and have knowledge about your data.\\nQuestion Answering: The second big LangChain use case. Answering questions over specific documents, only utilizing the information in those documents to construct an answer.\\nChatbots: Since language models are good at producing text, that makes them ideal for creating chatbots.\\nQuerying Tabular Data: If you want to understand how to\n",
            "\n",
            "-----\n",
            "\n",
            "how do I use the LLMChain in LangChain?\n"
          ]
        }
      ],
      "source": [
        "print(augmented_query)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sihH_GMiV5_p"
      },
      "source": [
        "Now we ask the question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IThBqBi8V70d"
      },
      "outputs": [],
      "source": [
        "# system message to 'prime' the model\n",
        "primer = f\"\"\"You are Q&A bot. A highly intelligent system that answers\n",
        "user questions based on the information provided by the user above\n",
        "each question. If the information can not be found in the information\n",
        "provided by the user you truthfully say \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": augmented_query}\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvS1yJhOWpiJ"
      },
      "source": [
        "To display this response nicely, we will display it in markdown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 64
        },
        "id": "RDo2qeMHWto1",
        "outputId": "3b17e6ac-10cc-4b8a-a1df-c4cdd172361b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "The information provided does not include specific instructions on how to use the LLMChain in LangChain. It would be beneficial to refer to the specific sections in the LangChain documentation or tutorial for this information."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown\n",
        "\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJ-a8MHg0eYQ"
      },
      "source": [
        "Let's compare this to a non-augmented query..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "vwhaSgdF0ZDX",
        "outputId": "13413a60-a175-4fe4-9665-26b1fc76a6a2"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry, but I don't have any information on how to use the LLMChain in LangChain."
          },
          "metadata": {}
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": primer},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CSsA-dW0m_P"
      },
      "source": [
        "If we drop the `\"I don't know\"` part of the `primer`?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "Z3svdTCZ0iJ2",
        "outputId": "86b75175-9a32-4855-e20f-68120fceff5a"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "I'm sorry for the confusion, but currently, there's no available information or data related to \"LLMChain in LangChain\". It may be a term associated with a specific setup or software configuration that is not widely known or established. \n\nI suggest you to double-check the terminology, or provide more detailed context so I can deliver a more precise answer. You may also want to try contacting the developer or visiting the official website or community of LangChain if there's one where you could possibly get more valuable insights and support."
          },
          "metadata": {}
        }
      ],
      "source": [
        "res = openai.ChatCompletion.create(\n",
        "    model=\"gpt-4\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are Q&A bot. A highly intelligent system that answers user questions\"},\n",
        "        {\"role\": \"user\", \"content\": query}\n",
        "    ]\n",
        ")\n",
        "display(Markdown(res['choices'][0]['message']['content']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcGon5672lBb"
      },
      "source": [
        "Then we see something even worse than `\"I don't know\"` ‚Äî hallucinations. Clearly augmenting our queries with additional context can make a huge difference to the performance of our system.\n",
        "\n",
        "Great, we've seen how to augment GPT-4 with semantic search to allow us to answer LangChain specific queries.\n",
        "\n",
        "Once you're finished, we delete the index to save resources."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ah_vfEHV2khx"
      },
      "outputs": [],
      "source": [
        "pinecone.delete_index(index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iEUMlO8M2h4Y"
      },
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}